%!TEX root=../mythesis.tex
% Chapter 1

\chapter{Introduction} % Main chapter title
\chaptermark{Introduction}
\label{ch:chapter1} 
\section{Preamble}\label{sec:preamble}
% 软件演进是不可避免的，软件演进以前由人来完成，非常的费力且容易出错，以前有很多帮助软件演进的工作。
% AI为软件演进带来了新的可能性。另外modern software developement范式也发生了改变。出现了很多AI工具来提升开发者的效率甚至是替代开发者直接参与到软件演进的过程中。coderabbit，AIDE等等。
% AI在软件开发生命周期的各个阶段都发挥着重要作用，从需求分析、设计、编码、测试到维护和演进，AI工具正在改变软件开发的方式和效率。但是AI的生成式特性和软件系统的复杂性使得AI在软件演进中的应用面临着可靠性和安全性的挑战。AI生成的代码可能包含错误或漏洞，尤其是在处理复杂的软件系统时，这些问题可能会被放大，导致严重的后果。因此，确保AI辅助的软件演进的可靠性和安全性成为一个重要的研究问题。
% AI的幻觉始终是一个隐患，

% preamble
% 1.软件演进：介绍什么是软件演进，为什么重要，为什么不可避免
% 2.软件演进的管理为什么困难 + survey (statistics)
% 3.AI是什么，AI Agent是什么，AI开始重塑软件开发和软件演进的各个环节。可以介绍AIDE等
% 4.但是AI事实上缺少严格的逻辑推理和依然依赖概率，这带来了很多新的问题和挑战。
% 5.过去的研究做了什么努力，数据集上方法上，例如swe-benchmark，以及别的research。
% 6.本文是为了更好地利用AI的能力并帮助软件更安全更可靠地演进，从external和internal两个方向进行
% How AI assist software evolution? Gaps identified in this thesis
% 图：讲解文档，代码，依赖网络 / 开发者是监控重要一环
% 
% contribution
% 以上挑战暴露了AI在处理软件演进过程中的问题：1.AI在处理项目时忽略了其只是一个高度依赖网络中的一个节点，仍然缺少对复杂的依赖关系的理解。AI依赖于训练数据，因此对于library会导致的后果难以预测。2.缺少真正的逻辑推断能力，依然是概率游戏。
% 我们将站在client的视角将AI-assisted的软件演进的挑战大体分为external和internal。external的难点主要来源于数据的缺乏，训练数据直接影响AI对外部library对client项目的产生影响的理解。因此需要搞定数据。对于AI而言，internal和external的桥梁是documentation，因此需要保持code-documentation的一致性。人作为重要的监控者，文档也是极其重要的接口。对代码进行演进时最重要的前置步骤是代码定位，但是现在的AI真的可以很好地胜任这一过程吗？





In the context of rapid digital transformation and the accelerated advancement of Artificial Intelligence (AI), software systems have evolved from relatively static artifacts into dynamic entities. To remain effective in the face of changing requirements, technological innovations, and emerging security threats, ongoing modification and extension, collectively known as software evolution, have become an inherent and unavoidable characteristic of modern software engineering. Broadly, software evolution is primarily driven by two forces: \textbf{external adaptation}, which responds to changes in the third-party ecosystem, and \textbf{internal improvement}, which necessitates the synchronous co-evolution of implementation logic (code) and semantic interface (documentation) to satisfy evolving functional and quality requirements.

The deep integration of AI, especially Large Language Models (LLMs), into the software development lifecycle has signaled a significant paradigm shift in software evolution. Empowered by advances in artificial intelligence, intelligent development tools and autonomous agents are increasingly capable of understanding and processing large-scale codebases alongside natural language requirements, while progressively demonstrating higher-level reasoning abilities such as cross-context analysis and complicated problem decomposition. Leveraging these capabilities, AI systems can automate a wide range of labor-intensive and high-complexity tasks, such as code generation, bug fixing, documentation synchronization, and system maintenance, thereby reshaping critical stages of software evolution and enabling complex software systems to achieve continuous iteration and scalable growth at unprecedented speed. Meanwhile, this AI-driven development paradigm introduces new technical pathways for managing long-term software evolution, propelling software engineering beyond human-centered development model toward a new stage characterized by the coexistence of human–AI collaboration and autonomous intelligence.

Despite this potential, the application of LLMs in software evolution remains fundamentally constrained by their probabilistic generative nature. Although large-scale parameterization facilitates the emergence of complex reasoning capabilities, LLMs generate outputs based on learned statistical patterns rather than deterministic logic, making it a probabilistic mirage that lacks the deterministic guarantees required for reliability. At the same time, modern software is no longer an isolated entity but a node in a highly interconnected dependency network: a project is not only an internal system but also a third-party library for other projects. Allowing AI agents to arbitrarily modify code and documentation is highly risky, may inadvertently introduce new vulnerabilities or disrupt existing functionalities. In this context, even seemingly minor modifications, such as a library upgrade or internal refactoring, can propagate cascading effects that compromise system reliability, security, and maintainability at scale. 

Consequently, ensuring the reliability and robustness throughout AI-assisted software evolution can be distilled into three interrelated core challenges:

\begin{itemize}
  \item \textbf{Managing external evolutionary risks.} Reliable external adaptation is predicated on the ability of AI to foresee and mitigate the risks from third-party library upgrades. However, the scarcity of high-quality, real-world datasets that captures intricate patterns of breaking changes caused by third-party library upgrades prevents AI from evaluating and solving external dependency risks.
  \item \textbf{Maintaining internal semantic alignment.} Internal improvement requires the synchronized update of code and documentation. In practice, code and documentation are often updated asynchronously, leading to semantic drift. Since documentation serves as the outward interface for a project's dependencies, these internal inconsistencies directly amplify external usage risks. 
  \item \textbf{Achieving precise evolution execution.} Precise code localization is the prerequisite for all automated evolution tasks. However, existing LLM-based approaches still rely heavily on superficial textual cues rather than deep structural reasoning. Imprecise identification of modification scopes increases downstream overhead, raises the risk of unintended consequences, and ultimately degrades overall evolution quality.
\end{itemize}

These three challenges correspond respectively to external adaptation, internal alignment, and execution precision, together forming the central bottleneck in AI-enabled software evolution and providing the systematic research motivation and theoretical foundation for this dissertation.

\section{Contributions}\label{sec:contribution}
To address these multifaceted challenges, this dissertation advocates for a systematic and heterogeneous set of solutions. For external adaptation, we prioritize empirical grounding by constructing high-quality datasets that capture real-world breaking change patterns, thereby providing a verifiable basis for AI systems to perceive and reason about evolutionary risks. For internal improvement, our strategy is two-pronged. On one hand, we develop a multi-parameter inconsistency detection framework which combines formal engine with LLM to ensure the semantic integrity between code and documentation. On the other hand, we first formalize the \textit{Keyword Shortcut} bias in current code localization research and propose a novel neurosymbolic agent framework that offloads reasoning to a logic engine to transcend the limitations of textual context in repository-level code localization. 

Central to these solutions is the paradigm of neurosymbolic AI, which serves as the methodological foundation of this research. By synthesizing the formal rigor of symbolic logic, such as symbolic execution and Datalog, with the adaptive learning power of Large Language Models, we provide a deterministic anchor for AI-driven evolution, ensuring that complex software modifications are not only contextually aware but also logically verifiable. \fref{fig:thesis_structure} illustrates the structure of this thesis, which is organized into six chapters. The core contributions of this dissertation are summarized as follows:

\begin{itemize}
  \item \textbf{Reproducible real-world incompatibility dataset.} We construct a dataset, \toolcompsuite, including 123 reproducible, real-world client-library pairs that manifest incompatibility issues when upgrading the library. These data points originate from 88 clients and 104 libraries. We created an automated command-line interface for the dataset. With this interface, users are able to programmatically replicate an incompatibility issue from the dataset with a single command. The interface also offers separate commands for each step involved in the reproduction of incompatibility issues.
  \item \textbf{Multi-parameter code–documentation inconsistency detection.} We proposed an automated multi-parameter code-documentation inconsistency detection technique and developed an end-to-end command-line tool called \toolchecker. Existing techniques in the same area are only designed to handle single parameter inconsistencies, without considering inter-parameter constraints. We introduced a customized fuzzy constraint satisfaction framework to mitigate the uncertainties introduced by LLM outputs. We provide a theoretical derivation of the membership function based on constraint similarity. We constructed a documentation constraint dataset comprising 72 real-world constraints sourced from widely used data science libraries, and derived a mutation-based inconsistency dataset with 216 constraints. Our dataset and tool implementation are made available online: \url{https://github.com/ParsifalXu/MPChecker}. We evaluated our tool on four real-world popular data science libraries. We reported 14 inconsistency issues discovered by \toolchecker to the developers, who have confirmed 11 inconsistencies at the time of writing.
  \item \textbf{Formalization of Keyword Shortcut bias and diagnostic benchmark.} We identify and formalize the \textit{Keyword Shortcut} bias in current code localization research. To address this, we introduce \dataset, a diagnostic benchmark specifically designed for Keyword-Agnostic Logical Code Localization (KA-LCL). It contains 25 high-quality purely logical queries with precise ground-truth locations, providing a rigorous testing ground for evaluating the structural reasoning capabilities of LLMs and AI agents. We also introduce \negset, a variant of \dataset where queries are intentionally modified to ensure their ground-truth sets are empty, to evaluate the abstention capability when no valid location meets the query.
  \item \textbf{Neurosymbolic agent framework for repository-level code localization.} We proposed a novel agent-based framework for repo-level code localization that introduces program facts as an intermediate representation to capture both explicit and implicit code relationships. By synthesizing Datalog queries from natural language, \tooldataloc offloads intricate structural traversal to a high-performance deterministic reasoning engine, significantly enhancing reasoning capabilities and reducing token consumption. We implement our framework as an automated, end-to-end command-line tool. It features an iterative refinement mechanism where the LLM agent progressively generates and adjusts Datalog rules to navigate repositories. Our tool and benchmark are publicly available at: \url{https://anonymous.4open.science/r/DataLoc-EFF3}. We conduct an extensive evaluation of \tooldataloc on both \dataset and other issue-driven benchmarks. The experimental results demonstrate that \tooldataloc significantly outperforms state-of-the-art methods in KA-LCL tasks, achieving superior precision and the capacity for verifiable localization. Furthermore, \tooldataloc maintains competitive performance on standard issue-driven benchmarks, matching SOTA levels while offering higher reliability in handling negative queries through its deterministic logic.
\end{itemize}

\begin{figure}[htbp]

\begin{tikzpicture}[
    node distance = 1.5cm and 0.2em,
    chapter_node/.style = {
        align=center,
        font=\sffamily\small,
        % text width=0.28\textwidth,
        inner sep=2pt,
    },
    arrow/.style = {
        -{Stealth[scale=1.2]},
        thin
    }
]

    \node[chapter_node] (c1) {
        \textcolor{blue}{\hyperref[ch:chapter1]{\underline{\textbf{Chapter 1}}}} \\ 
        Introduction
    };

    \node[chapter_node, below=of c1] (c2) {
        \textcolor{blue}{\hyperref[ch:chapter2]{\underline{\textbf{Chapter 2}}}} \\
        Background and Literature Review
    };

    \node[chapter_node, below=1.5cm of c2] (c4) {
        \textcolor{blue}{\hyperref[ch:chapter4]{\underline{\textbf{Chapter 4}}}} \\
        \textbf{\textsc{MPChecker}:} Multi-parameter \\ 
        Code-Doc Inconsistency Detection\\ 
        \textcolor{blue}{$\bullet$ new approach}
    };

    \node[chapter_node, left= of c4] (c3) {
        \textcolor{blue}{\hyperref[ch:chapter3]{\underline{\textbf{Chapter 3}}}} \\
        \textbf{\textsc{CompSuite}:} A Dataset of Lib-\\ 
        rary Upgrade Incompatibilities\\
        \textcolor{blue}{$\bullet$ new dataset}
    };

    \node[chapter_node, right= of c4] (c5) {
        \textcolor{blue}{\hyperref[ch:chapter5]{\underline{\textbf{Chapter 5}}}} \\
        \textbf{\textsc{DataLoc}:} Neurosymbolic \\ 
        Repo-level Code Localization\\ 
        \textcolor{blue}{$\bullet$ findings \& new framework}
    };

    \node[chapter_node, below=1.5cm of c4] (c6) {
        \textcolor{blue}{\hyperref[ch:chapter6]{\underline{\textbf{Chapter 6}}}} \\
        Conclusion and Future Work
    };

    \draw[arrow] (c1.south) -- (c2.north);

    \coordinate (split) at ($(c2.south)!0.3!(c4.north)$);
    \draw (c2.south) -- (split); 

    \draw[arrow] (split) .. controls ++(0,-1.2) and ++(0,1.2) .. (c3.north);
    \draw[arrow] (split) -- (c4.north);
    \draw[arrow] (split) .. controls ++(0,-1.2) and ++(0,1.2) .. (c5.north);

    \coordinate (merge) at ($(c4.south)!0.7!(c6.north)$);

    \draw[arrow] (merge) -- (c6.north); 
    \draw (c3.south) .. controls ++(0,-1.2) and ++(0,1.2) .. (merge);
    \draw (c4.south) -- (merge);
    \draw (c5.south) .. controls ++(0,-1.2) and ++(0,1.2) .. (merge);

\end{tikzpicture}
\caption{The Structure of the Thesis}\label{fig:thesis_structure}
\end{figure}


\section{Organization}\label{sec:organization}
The rest of the thesis is organized as follows:

\begin{itemize}
  \item \hyperref[ch:chapter2]{\underline{\textbf{\cref{ch:chapter2}}}} provides essential background and key concepts necessary for understanding the thesis.
  \item \hyperref[ch:chapter3]{\underline{\textbf{\cref{ch:chapter3}}}} presents \toolcompsuite, a dataset of real-world library upgrade incompatibilities, detailing its construction, characteristics, and potential applications.
  \item \hyperref[ch:chapter4]{\underline{\textbf{\cref{ch:chapter4}}}} introduces \toolchecker, a multi-parameter code-documentation inconsistency detection tool, describing its design, implementation, and evaluation on real-world libraries.
  \item \hyperref[ch:chapter5]{\underline{\textbf{\cref{ch:chapter5}}}} formalizes the keyword shortcut bias in code localization, introduces diagnostic benchmarks, \dataset and \negset, and presents \tooldataloc, a neurosymbolic agent framework for repository-level code localization, along with its evaluation results.
  \item \hyperref[ch:chapter6]{\underline{\textbf{\cref{ch:chapter6}}}} concludes the thesis by summarizing the contributions, discussing the implications of the findings, and outlining directions for future research in AI-enabled software evolution.
\end{itemize}
