\section{Introduction}\label{sec:intro-dataloc}
The rapid evolution of artificial intelligence has fundamentally reshaped every phase of the Software Development Life Cycle (SDLC), revolutionizing developer-codebase interactions. Through natural language, developers can now gain comprehensive insight into code repositories and perform sophisticated tasks such as code refactoring, feature implementation, and defect repair. At the heart of these capabilities lies the critical primitive of \textit{code localization}. Formally, code localization is the process of precisely identifying relevant source code snippets, ranging from specific methods to complex logic blocks, within a large-scale code repository that aligns with a natural language query. As the community pivots towards autonomous software engineering agents, the efficacy of code localization has emerged as the primary bottleneck. Accurately mapping high-level functional intent to intricate implementation logic is no longer merely a retrieval challenge, but an essential prerequisite for AI systems to reliably navigate and reason about real-world software architectures.

Contemporary state-of-the-art code localization approaches, which can be broadly categorized into three paradigms including embedding-based retrieval, pipeline-guided LLM workflows, and graph-augmented agentic exploration (Details in Section~\ref{sec:related}), have reported impressive results on issue-solving benchmarks such as SWE-bench~\cite{swebench2024}. However, our investigation reveals a significant but overlooked bias that we term the \textbf{Keyword Shortcut}. Recent mainstream benchmarks are predominantly curated from GitHub issues, which usually contain clear error traces or even verbatim code snippets. Such descriptions are inherently laden with \textbf{keywords} (e.g., precise class names or unique identifiers) that act as ``cheat sheets'', allowing models to locate code snippets via surface-level lexical matching rather than genuine logical reasoning. Our preliminary diagnostic study indicates that once these identifiers are stripped away, the performance of all three approaches suffers a catastrophic decline. This stark contrast uncovers a fundamental deficiency in current systems: a profound struggle with \textbf{Keyword-Agnostic Logical Code Localization (KA-LCL)}, where models must navigate codebases without the crutch of explicit naming hints. It is crucial to distinguish KA-LCL from general issue-solving code localization: while the latter is often reducible to a \textit{semantic matching} task between query keywords and code identifiers, KA-LCL represents a higher-order \textit{structural reasoning} challenge. To illustrate this, consider a seemingly straightforward structural logical query: \textit{``Find all functions where: (1) the function has more than 15 parameters, and (2) the function is not an \_\_init\_\_ method''} (Details in Section~\ref{sec:example}). Such an keyword-agnostic logical query poses a significant hurdle for SOTA approaches. While a human developer can easily identify these patterns by traversing the program's structural logic, SOTA solutions frequently fail because they cannot rely on semantic similarity to specific identifiers. Instead, these queries necessitate a deeper understanding of code structures, where existing approaches, deprived of naming cues, prove remarkably brittle.

To systematically evaluate the limits of existing tools, we first introduce \dataset, a diagnostic benchmark specifically curated for keyword-agnostic logical code localization. Unlike widely used benchmarks that take issue statements saturated with naming hints, \dataset targets scenarios where no key entities serve as anchors. By decomposing code structures, we synthesized a series of purely logical queries that focus on code patterns. Our diagnostic evaluation reveals a precipitous performance degradation in state-of-the-art methods, uncovering a critical ``reasoning gap'' that current AI-driven localization methods have yet to bridge.

The difficulty of KA-LCL arises from two intertwined technical bottlenecks that current paradigms are ill-equipped to handle. \ding{182} The absence of lexical anchors leads to an unmanageable search space, significantly exacerbating the \textit{lost-in-the-middle} phenomenon. Modern software systems are typically massive and complex, and model-based approaches rely heavily on specific identifiers as ``pruning signals'' to filter out irrelevant modules. Without these \textit{keyword shortcuts}, models are forced to ingest a vast volume of structural context to identify potential candidates. This data deluge overwhelms the limited context window of LLMs, where critical logical patterns become submerged within long input sequences, severely impairing the system's ability to robustly access and utilize relevant structural features. \ding{183} Existing approaches lack a deterministic reasoning mechanism to navigate the intricate hierarchical dependencies of a repository-level codebase. While pipeline-guided LLM workflows and graph-augmented agentic exploration attempt to address code relationships, they often operate as probabilistic recommendation systems that generate a ranked list of likely candidates, rather than performing rigorous structural deduction. Consequently, the lack of a formal reasoning framework prevents these systems from providing either a deterministic localization or a verifiable explanation, failing to bridge the gap from heuristic-based matching to genuine repository-level logical inference.

Conceptually, code localization can be viewed as a specialized form of code search~\cite{di2023code}. However, unlike general information retrieval, programming languages possess formally defined syntax and semantics that allow source code to be precisely parsed and analyzed. This formal nature endows code with an inherent reasonability that extends beyond surface-level text. From a high-level perspective, an effective repository-level localization engine requires a robust intermediate representation (IR) to bridge the semantic gap between natural language intent and implementation logic. Such an IR must effectively encode code entities, their intricate inter-relationships, and structural hierarchies, while remaining highly interpretable and actionable for LLM-based agents.

To overcome the aforementioned limitations, we propose \tooldataloc, a novel agentic framework that synergizes the rule-based reasoning of \textbf{Datalog} with the semantic power of LLMs to achieve precise, repository-level code localization. Our framework first employs static analysis to extract a comprehensive set of \textbf{program facts} from the source code, constructing a structured IR that captures both elemental properties and relational dependencies. Upon receiving a natural language query, the LLM agent interprets the underlying functional intent and synthesizes a corresponding Datalog query. As a powerful declarative logic programming language, Datalog is uniquely suited for traversing complex structural patterns that baffle traditional retrieval methods. These queries are then executed by \textbf{Souffl√©}, a high-performance reasoning engine, which performs rigorous deduction against the pre-extracted facts to infer precise code locations. Crucially, by offloading structural reasoning to a deterministic engine, \tooldataloc not only significantly reduces token consumption but also empowers the agent to provide definitive negative responses when no matches exist. This avoids the common pitfall of probabilistic systems that hallucinate potential candidates, thereby achieving a paradigm shift from heuristic-based recommendation to verifiable, high-precision localization.

\textbf{Contributions.} Our work aims to integrate Datalog's rule-based inference engine with the advanced large language models. This framework embodies an exploration of the neuro-symbolic paradigm and hope to contribute to open science. In summary, we make the following contributions.

\begin{enumerate}
    \item We identify and formalize the \textit{Keyword Shortcut} bias in current code localization research. To address this, we introduce \dataset, a diagnostic benchmark specifically designed for Keyword-Agnostic Logical Code Localization (KA-LCL). It contains 25 high-quality purely logical queries with precise ground-truth locations, providing a rigorous testing ground for evaluating the structural reasoning capabilities of LLMs and AI agents.
    \item We proposed a novel agent-based framework for repo-level code localization that introduces program facts as an intermediate representation to capture both explicit and implicit code relationships. By synthesizing Datalog queries from natural language, \tooldataloc offloads intricate structural traversal to a high-performance deterministic reasoning engine, significantly enhancing reasoning capabilities and reducing token consumption.
    \item We implement our framework as an automated, end-to-end command-line tool. It features an iterative refinement mechanism where the LLM agent progressively generates and adjusts Datalog rules to navigate repositories. Our tool and benchmark are publicly available at: \url{https://anonymous.4open.science/r/DataLoc-EFF3}.
    \item We conduct an extensive evaluation of \tooldataloc on both \dataset and other issue-driven benchmarks. The experimental results demonstrate that \tooldataloc significantly outperforms state-of-the-art methods in KA-LCL tasks, achieving superior precision and the capacity for verifiable localization. Furthermore, \tooldataloc maintains competitive performance on standard issue-driven benchmarks, matching SOTA levels while offering higher reliability in handling negative queries through its deterministic logic.
\end{enumerate}



