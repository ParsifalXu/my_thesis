\section{Evaluation}\label{sec:eval}

To evaluate the effectiveness and practicality of our approach at repository-level, we design the following research questions:
\begin{enumerate}
	\item \textbf{RQ1:} How effective is \tooldataloc in keyword-agnostic logical code localization?
	\item \textbf{RQ2:} How effective is \tooldataloc for issue-based code localization?
	\item \textbf{RQ3:} How efficient is \tooldataloc compared to baselines?
    \item \textbf{RQ4:} How does each component of \tooldataloc contribute to its performance?
\end{enumerate}


\subsection{Experiment Setup}

\subsubsection{Benchmarks.} 
We evaluate code localization performance on three Python-based benchmarks, covering both complex logical reasoning challenges and industrial issue-resolution tasks.

\textbf{SWE-bench Lite}~\cite{swebench2024}. A carefully curated and widely recognized subset from the full SWE-bench for more efficient and cost-effective evaluation of autonomous issue-solving capabilities. It consists of real-world GitHub issues with repository metadata and ground-truth patch locations. Following Suresh et al.~\cite{suresh2024cornstack}, we retained 274 of 300 original instances where patches modify existing functions or classes. We intentionally excluded instances introducing code corresponding to new functions or import statements to focus the evaluation on code localization within existing structures.

\textbf{\dataset} (Ours). To evaluate the capability of localization approaches in keyword-agnostic logical code localization, we constructed \dataset, a diagnostic benchmark comprising 25 high-quality logic-intensive queries. As illustrated in Table~\ref{tab:code_dimensions}, each query is formulated as a composite logical proposition by integrating code features across multiple dimensions. By combining structural granularity (e.g., classes, methods) with behavioral attributes (e.g., control flow, exception handling) and code metrics (e.g., inheritance depth and branch count), \dataset captures complex patterns that demand deep repository understanding.

For each query, we utilize the environment (repository and base commit version) from the first case of SWE-bench Lite as the foundation. For each query, we executed searches using \texttt{Cursor} and \texttt{GitHub Copilot} in agent mode with multiple latest advanced models like Claude-4.5-Opus and GPT-5.2, and manually validated all returned results to establish the ground-truth locations.

\dataset serves as a critical complement to issue-based benchmarks for localization task. In practice, issues are one of the most important channels for error feedback between users and maintainers. To facilitate debugging, those issue descriptions often provide sufficient information and clear keywords as cues to help maintainers better locate faults, such as accurate file paths, function identifiers, or even specific code snippets. Our analysis of SWE-bench Lite instances reveals that over 50\% of ground-truth locations are mentioned in the issue descriptions. Such \textit{keyword shortcut} enables models to succeed via simple lexical matching (e.g. grep) or embedding-based retrieval, without requiring genuine understanding and reasoning over the codebase. This undermines the validity of localization performance evaluations. Moreover, LLM-assisted development shifts the codebase interaction toward intent-based question answering, allowing developers to query repositories using natural language. However, for developers unfamiliar with a given repository, they typically cannot use precise identifiers and instead tend to express their search intent through high-level behavioral pattern descriptions or abstract logical structures. 


\textbf{\negset} (Ours). \negset is a variant of \dataset where queries are intentionally modified to ensure their ground-truth sets are empty. Current methods often adopt top-$n$ ranking to maximize recall, but ideal robust localization requires the ability to provide ascertained answers and avoid false positives. \negset evaluates the abstention capability when no valid location meets the query. Such ``refusal'' mechanism is a critical metric for ensuring the reliability of autonomous agents in production environments.


\begin{table}[htbp]
\centering
\caption{Taxonomy of Python Code Dimensions and Representative Elements}
\label{tab:code_dimensions}
\small
\renewcommand{\arraystretch}{1.2}
\begin{tabularx}{\textwidth}{lX}
\toprule
\textbf{Query Dimensions} & \textbf{Examples / Typical Elements} \\ \midrule
Code Structure & Functions, Methods, Classes, Modules, Decorators \\
Control Flow & Conditional (\texttt{if-elif-else}), Iteration (\texttt{for}, \texttt{while}), Context Management (\texttt{with}) \\
Condition Logic & Comparison (\texttt{==}, \texttt{>}), Identity (\texttt{is}), Membership (\texttt{in}), Type Checks (\texttt{isinstance}), Logical Operators (\texttt{and}, \texttt{or}, \texttt{not}), Early Exit (\texttt{return}, \texttt{break}, \texttt{continue}) \\
Data Structure & Built-in Collections (\texttt{list}, \texttt{dict}, \texttt{set}), Primitive Types (\texttt{int}, \texttt{str}) \\
Function Signatures & Default Values, Variadic Parameters (\texttt{*args}, \texttt{**kwargs}), Type Annotations \\
Exception Handling & Exception Propagation (\texttt{try-except-finally}), Exceptions (\texttt{TypeError}) \\
Code Metrics & Nesting Depth, Inheritance Depth, Assertion Count, Branch count \\ \bottomrule
\end{tabularx}
\end{table}

% To assess the effectiveness of our framework, we require datasets containing ground-truth annotations for code localization that can serve as reliable and standardized benchmarks. SWE-bench~\cite{jimenez2024swebench} constitutes a widely adopted benchmark for evaluating the capabilities of AI systems in performing end-to-end bug fixing across repository-level codebases. Each instance consists of a GitHub issue paired with its corresponding code patches. SWE-bench Lite~\cite{swebench2024} provides a carefully curated subset of 300 tasks from the full benchmark, designed to reduce evaluation costs while preserving the benchmark’s representativeness and overall quality. Following the approach of Suresh et al.~\cite{suresh2024cornstack}, we retained 274 of the 300 instances where patches modify existing functions or classes, excluding instances that introduce new functions or import statements. As code localization constitutes a critical yet implicit intermediate step in bug fixing process, we use the modified code locations from the patches as ground truth to evaluate localization performance.

% Similar to Swe-Bench, LocBench~\cite{chen-etal-2025-locagent}, proposed by Chen et al., is a dataset specifically designed for code localization, comprising 560 issues from Python repositories. Collected after October 2024 to mitigate data leakage and pre-training bias in recent LLMs, it encompasses a broad range of issue categories beyond bug fixing. After removing inaccessible repositories, we retain xxx of the original 560 examples. In summary, for Python, we adopt both SWE-bench Lite and LocBench.

% Our framework supports not only Python but also Java. For Java evaluation, we extract relevant instances from two multilingual datasets: SWE-bench Multilingual~\cite{yang2025swesmith}, which provides 300 tasks across 42 repositories and 9 programming languages including Java, and Multi-SWE-bench~\cite{zan2025multiswebench} from ByteDance, spanning 7 languages with 1,632 high-quality instances. After extracting Java-related entries and deduplicating across both sources, the combined dataset contains XXX instances. We manually annotate code locations as ground truth based on the \texttt{git diff} information, forming a dataset we refer to as SWE-bench Java.

% \todo{average file change, class change, function change, etc.}

\subsubsection{Baselines.} To assess \tooldataloc, we select four state-of-the-art baselines representing three distinct technical paradigms: embedding-based, pipeline-based, and agent-based approaches:
\begin{enumerate}[leftmargin=*]
    \item \textbf{SweRank}~\cite{reddy2025swerank} (\textit{Embedding-based}): It utilizes a retrieve-and-rerank architecture to identify issue locations. It employs SWERankEmbed (137M/7B parameters) to perform initial retrieval and SWERankLLM (7B/32B parameters) to rerank the results.
    \item \textbf{Agentless}~\cite{xia2024agentless} (\textit{Pipeline-based}): This approaches employs a hierarchical filtering strategy within a procedural workflow. It progressively prunes the search space from the file level down to specific classes or functions, utilizing an LLM to rank and select candidates at each stage.
    \item \textbf{LocAgent}~\cite{chen-etal-2025-locagent} (\textit{Agent-based}): It constructs a graph-based representation and sparse indexes of the project and enable an autonomous agent to perform iterative, tool-assisted retrieval.
    \item \textbf{CoSIL}~\cite{jiang2025cosil} (\textit{Agent-based}): This framework focuses on structural dependency traversal through call graphs to identity implicit locations via iterative exploration. It incorporates pruning to maintain context efficiency and restrict the search to high-relevance execution paths.
    % \item \textbf{Orca Loca~\cite{yu2025orcalocallmagentframework}:} Integrates priority-based scheduling, action decomposition with relevance scoring, and distance-aware context pruning. By optimizing the synergy between agentic reasoning and precise retrieval, it effectively navigates complex repositories to resolve the suboptimality of current search mechanisms.
\end{enumerate}

\subsection{Metrics}
We evaluate localization performance at three granularity: \textit{file, module, and function}. Let $Q$ denote the set of query instances, $G_q$ the set of ground-truth locations for query $q \in Q$, and $\mathcal{P}_q$ the set of predicted locations inferred by our agent workflow. $\mathbf{1}(\cdot)$ denotes the \textbf{indicator function}, which equals 1 if the logical condition holds and 0 otherwise. We adopt the following six metrics:

\begin{enumerate}[leftmargin=*, label=\textbf{M\arabic*.}]
    \item \textbf{Accuracy@k (ACC@k):} It measures the ability to achieve full coverage, where a success requires all ground-truth locations to be present within the top-$k$ predicted locations. When $k$ equals the length of the prediction set, this metric becomes the \textbf{Success Rate (SR)}:
    \begin{equation}
        Acc@k = \frac{1}{|Q|} \sum_{q \in Q} \mathbf{1}(G_q \subseteq \mathcal{P}_{q,k})
    \end{equation}

    \item \textbf{Recall (REC):} It represents the proportion of ground-truth locations successfully captured by the predicted set $\mathcal{P}_q$:
    \begin{equation}
        Rec@k = \frac{1}{|Q|} \sum_{q \in Q} \frac{|G_q \cap \mathcal{P}_{q,k}|}{|G_q|}
    \end{equation}

    \item \textbf{Precision (PRE):} This metric penalizes overprediction by calculating the fraction of predicted locations that are correct:
    \begin{equation}
        Pre = \frac{1}{|Q|} \sum_{q \in Q} \frac{|G_q \cap \mathcal{P}_q|}{|\mathcal{P}_q|}
    \end{equation}

    \item \textbf{Average Jaccard Similarity (AJS):} It quantifies the overlap between the predicted and ground-truth sets, which penalizes both missing targets and redundant predictions:
    \begin{equation}
        AJS = \frac{1}{|Q|} \sum_{q \in Q} \frac{|G_q \cap \mathcal{P}_q|}{|G_q \cup \mathcal{P}_q|}
    \end{equation}

    \item \textbf{Perfect Location Rate (PLR):} The most Stringent metric, measuring the ratio of instances where the predicted set $\mathcal{P}_q$ exactly matches the ground-truth set $G_q$. A PLR of 1.0 indicates perfect localization without any extraneous noise (i.e., $AJS = 1.0$):
    \begin{equation}
        PLR = \frac{1}{|Q|} \sum_{q \in Q} \mathbf{1}(\mathcal{P}_q = G_q)
    \end{equation}

    \item \textbf{Hit Rate (HR):} The most lenient metric, measuring the ratio of instances where the predicted set $\mathcal{P}_q$ provides at least one correct location:
    \begin{equation}
        HR = \frac{1}{|Q|} \sum_{q \in Q} \mathbf{1}(\mathcal{P}_{q} \cap G_q \neq \emptyset)
    \end{equation}
\end{enumerate}




\subsubsection{Implementation and environment.} All experiments were conducted on a server equipped with an Intel Xeon Silver 4216 CPU (2.10 GHz) and 62 GB RAM, running Ubuntu 22.04.5 LTS. Our framework was implemented using Python 3.12.11 and the Soufflé 2.4 Datalog engine. To evaluate \tooldataloc, we accessed \texttt{gpt-4o-20240513} via OpenAI’s API, \texttt{claude-3-5-sonnet-20241022} through AWS Bedrock services, \texttt{Deepseek-reasoner} via DeepSeek' API, and \texttt{Qwen3-Max} via Alibaba Cloud Service. For baseline comparisons, we instantiated runtime environments according to their respective official specifications and dependency requirements to ensure a fair evaluation.


\subsection{Results}

\input{Chapters/Chapter6/041.rq1}
\input{Chapters/Chapter6/042.rq2}
\input{Chapters/Chapter6/043.rq3}
\input{Chapters/Chapter6/044.rq4}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
